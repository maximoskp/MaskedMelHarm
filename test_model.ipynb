{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3036714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MusicEncoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 chord_vocab_size,  # V\n",
    "                 d_model=512, \n",
    "                 nhead=8, \n",
    "                 num_layers=6, \n",
    "                 dim_feedforward=2048,\n",
    "                 conditioning_dim=16, \n",
    "                 device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = 513  # 1 + 256 + 256\n",
    "\n",
    "        # Embedding for condition vector (e.g., style, time sig)\n",
    "        self.condition_proj = nn.Linear(conditioning_dim, d_model)\n",
    "\n",
    "        # Melody projection: 140D binary -> d_model\n",
    "        self.melody_proj = nn.Linear(140, d_model)\n",
    "\n",
    "        # Harmony token embedding: V -> d_model\n",
    "        self.harmony_embedding = nn.Embedding(chord_vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.seq_len, d_model))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, \n",
    "                                                   nhead=nhead, \n",
    "                                                   dim_feedforward=dim_feedforward,\n",
    "                                                   batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Optional: output head for harmonies\n",
    "        self.output_head = nn.Linear(d_model, chord_vocab_size)\n",
    "\n",
    "    def forward(self, conditioning_vec, melody_grid, harmony_tokens=None):\n",
    "        \"\"\"\n",
    "        conditioning_vec: (B, C)\n",
    "        melody_grid: (B, 256, 140)\n",
    "        harmony_tokens: (B, 256) - optional for training or inference\n",
    "        \"\"\"\n",
    "        B = conditioning_vec.size(0)\n",
    "\n",
    "        # Project condition: (B, d_model) → (B, 1, d_model)\n",
    "        cond_emb = self.condition_proj(conditioning_vec).unsqueeze(1)\n",
    "\n",
    "        # Project melody: (B, 256, 140) → (B, 256, d_model)\n",
    "        melody_emb = self.melody_proj(melody_grid)\n",
    "\n",
    "        # Harmony token embedding (optional for training): (B, 256) → (B, 256, d_model)\n",
    "        if harmony_tokens is not None:\n",
    "            harmony_emb = self.harmony_embedding(harmony_tokens)\n",
    "        else:\n",
    "            # Placeholder (zeros) if not provided\n",
    "            harmony_emb = torch.zeros(B, 256, self.d_model, device=self.device)\n",
    "\n",
    "        # Concatenate full input: (B, 1 + 256 + 256, d_model)\n",
    "        full_seq = torch.cat([cond_emb, melody_emb, harmony_emb], dim=1)\n",
    "\n",
    "        # Add positional encoding\n",
    "        full_seq = full_seq + self.pos_embedding[:, :self.seq_len, :]\n",
    "\n",
    "        # Transformer encode\n",
    "        encoded = self.encoder(full_seq)\n",
    "\n",
    "        # Optionally decode harmony logits (only last 256 tokens)\n",
    "        harmony_output = self.output_head(encoded[:, -256:, :])  # (B, 256, V)\n",
    "\n",
    "        return harmony_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd5b7486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256, 500])\n"
     ]
    }
   ],
   "source": [
    "model = MusicEncoderOnlyTransformer(chord_vocab_size=500, conditioning_dim=16)\n",
    "\n",
    "# Dummy batch\n",
    "batch_size = 4\n",
    "conditioning_vec = torch.randn(batch_size, 16)\n",
    "melody_grid = torch.randint(0, 2, (batch_size, 256, 140)).float()\n",
    "harmony_tokens = torch.randint(0, 500, (batch_size, 256))\n",
    "\n",
    "logits = model(conditioning_vec, melody_grid, harmony_tokens)\n",
    "print(logits.shape)  # (4, 256, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c157abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def apply_structured_masking(harmony_tokens, mask_token_id, step_idx, spacing_schedule):\n",
    "    \"\"\"\n",
    "    harmony_tokens: (B, 256) - original ground truth tokens\n",
    "    mask_token_id: int - ID for the special <mask> token\n",
    "    step_idx: int - 0, 1, 2, etc.\n",
    "    spacing_schedule: list[int] - e.g., [16, 8, 4]\n",
    "\n",
    "    Returns:\n",
    "        masked_harmony: (B, 256) with some tokens replaced with <mask>\n",
    "        target_harmony: (B, 256) with -100 at positions we do NOT want loss\n",
    "    \"\"\"\n",
    "    B, T = harmony_tokens.shape\n",
    "    assert T == 256\n",
    "\n",
    "    device = harmony_tokens.device\n",
    "    spacing = spacing_schedule[step_idx] if step_idx < len(spacing_schedule) else 1\n",
    "\n",
    "    # Get the indices that will remain unmasked for this step\n",
    "    mask = torch.ones((T,), dtype=torch.bool, device=device)  # start with all masked\n",
    "    mask[::spacing] = False  # reveal tokens at spacing\n",
    "\n",
    "    # Expand to batch\n",
    "    mask = mask.unsqueeze(0).expand(B, -1)  # shape (B, 256)\n",
    "\n",
    "    # Create masked version\n",
    "    masked_harmony = harmony_tokens.clone()\n",
    "    masked_harmony[mask] = mask_token_id\n",
    "\n",
    "    # Create target (loss computed only on masked positions)\n",
    "    target = harmony_tokens.clone()\n",
    "    target[~mask] = -100  # ignore tokens that were shown to the model\n",
    "\n",
    "    return masked_harmony, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca050b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, conditioning_vec, melody_grid, harmony_gt, step_idx, spacing_schedule, mask_token_id, loss_fn):\n",
    "    \"\"\"\n",
    "    - step_idx: controls the masking granularity\n",
    "    - model returns logits over vocab for harmony tokens\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # Mask harmony according to schedule\n",
    "    harmony_input, harmony_target = apply_structured_masking(\n",
    "        harmony_tokens=harmony_gt,\n",
    "        mask_token_id=mask_token_id,\n",
    "        step_idx=step_idx,\n",
    "        spacing_schedule=spacing_schedule\n",
    "    )\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(conditioning_vec, melody_grid, harmony_input)  # (B, 256, V)\n",
    "\n",
    "    # Compute loss only on masked tokens\n",
    "    loss = loss_fn(logits.view(-1, logits.size(-1)), harmony_target.view(-1))\n",
    "\n",
    "    # Backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd9a285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximos/miniconda3/envs/torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.00001)\n",
    "\n",
    "# Setup\n",
    "spacing_schedule = [16, 8, 4]  # define based on time signature\n",
    "mask_token_id = 499  # your chosen <mask> token in the vocab\n",
    "\n",
    "# Dummy training step\n",
    "step_idx = 1  # change dynamically for curriculum training\n",
    "loss = train_step(\n",
    "    model,\n",
    "    optimizer,\n",
    "    conditioning_vec,\n",
    "    melody_grid,\n",
    "    harmony_gt=harmony_tokens,\n",
    "    step_idx=step_idx,\n",
    "    spacing_schedule=spacing_schedule,\n",
    "    mask_token_id=mask_token_id,\n",
    "    loss_fn=loss_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_step_idx_linear(epoch, epochs_per_stage, max_step_idx):\n",
    "    return min(epoch // epochs_per_stage, max_step_idx)\n",
    "\n",
    "def get_step_idx_mixed(epoch, max_epoch, max_step_idx):\n",
    "    \"\"\"Returns a random step index, biased toward early stages in early epochs.\"\"\"\n",
    "    progress = epoch / max_epoch\n",
    "    probs = torch.softmax(torch.tensor([\n",
    "        (1.0 - abs(progress - (i / max_step_idx))) * 5 for i in range(max_step_idx + 1)\n",
    "    ]), dim=0)\n",
    "    return torch.multinomial(probs, 1).item()\n",
    "\n",
    "def train_with_curriculum(\n",
    "    model, optimizer, dataloader, loss_fn,\n",
    "    spacing_schedule, mask_token_id,\n",
    "    conditioning_encoder_fn,\n",
    "    epochs=10,\n",
    "    curriculum_type=\"linear\",  # \"linear\" or \"mixed\"\n",
    "    epochs_per_stage=2,\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    max_step_idx = len(spacing_schedule) - 1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Determine masking level\n",
    "        if curriculum_type == \"linear\":\n",
    "            step_idx = get_step_idx_linear(epoch, epochs_per_stage, max_step_idx)\n",
    "        elif curriculum_type == \"mixed\":\n",
    "            step_idx = get_step_idx_mixed(epoch, epochs, max_step_idx)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid curriculum type\")\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch} (step {step_idx})\"):\n",
    "            melody_grid = batch[\"melody_grid\"].to(device)           # (B, 256, 140)\n",
    "            harmony_gt = batch[\"harmony_tokens\"].to(device)         # (B, 256)\n",
    "            condition_features = batch[\"conditioning\"].to(device)  # (B, C0)\n",
    "\n",
    "            # Encode conditioning to vector\n",
    "            conditioning_vec = conditioning_encoder_fn(condition_features)  # (B, C)\n",
    "\n",
    "            # Apply masking to harmony\n",
    "            harmony_input, harmony_target = apply_structured_masking(\n",
    "                harmony_gt, mask_token_id, step_idx, spacing_schedule\n",
    "            )\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(conditioning_vec, melody_grid, harmony_input)\n",
    "\n",
    "            # Compute loss only on masked tokens\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), harmony_target.view(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch} | Avg Loss: {epoch_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fdf2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing_schedule = [16, 8, 4]\n",
    "mask_token_id = 499  # <mask> token index\n",
    "\n",
    "train_with_curriculum(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    dataloader=train_loader,\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(ignore_index=-100),\n",
    "    spacing_schedule=spacing_schedule,\n",
    "    mask_token_id=mask_token_id,\n",
    "    conditioning_encoder_fn=your_encoder_fn,  # maps e.g. style+time signature to (B, C)\n",
    "    epochs=20,\n",
    "    curriculum_type=\"mixed\",  # or \"linear\"\n",
    "    epochs_per_stage=3        # only for linear\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
